{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ],
      "metadata": {
        "id": "SNHW8BAg96Qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Al2M2mvyOT0M"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "          nn.BatchNorm2d(num_features=64),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "          nn.BatchNorm2d(num_features=64),\n",
        "          nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.block(x)\n",
        "        x = x + z\n",
        "        return x\n",
        "\n",
        "\n",
        "class SRGAN_g(nn.Module):\n",
        "    \"\"\" Generator in Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n",
        "    feature maps (n) and stride (s) feature maps (n) and stride (s)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SRGAN_g, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.residual_block = self.make_layer()\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='bilinear'),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=128, out_channels=64, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=(1, 1), stride=(1, 1)),\n",
        "        )\n",
        "\n",
        "    def make_layer(self):\n",
        "        layer_list = OrderedDict()\n",
        "        for i in range(10):\n",
        "            layer_list['res_block'+str(i+1)] = ResidualBlock()\n",
        "\n",
        "        return nn.Sequential(layer_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        temp = x\n",
        "        x = self.residual_block(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x + temp\n",
        "        x = torch.sigmoid(self.upsample(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class SRGAN_d(nn.Module):\n",
        "\n",
        "    def __init__(self, dim=64):\n",
        "        super(SRGAN_d, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=dim, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=dim, out_channels=dim * 2, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
        "            nn.BatchNorm2d(num_features=dim * 2),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=dim * 2, out_channels=dim * 4, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
        "            nn.BatchNorm2d(num_features=dim * 4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=dim * 4, out_channels=dim * 4, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
        "            nn.BatchNorm2d(num_features=dim * 4),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=dim * 4, out_channels=dim * 8, kernel_size=(4, 4), stride=(2, 2), padding=1),\n",
        "            nn.BatchNorm2d(num_features=dim * 8),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv2d(in_channels=dim * 8, out_channels=dim, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "            nn.BatchNorm2d(num_features=dim),\n",
        "            nn.LeakyReLU()\n",
        "        )\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "        self.dense = nn.Sequential(\n",
        "            nn.Linear(in_features=9216, out_features=4608),\n",
        "            nn.Linear(in_features=4608, out_features=1),\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.block(x)\n",
        "        x = self.flat(x)\n",
        "        x = self.dense(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDpppnjMQkNO"
      },
      "outputs": [],
      "source": [
        "__all__ = [\n",
        "    'VGG',\n",
        "    'vgg16',\n",
        "    'vgg19',\n",
        "    'VGG16',\n",
        "    'VGG19',\n",
        "]\n",
        "\n",
        "layer_names = [\n",
        "    ['conv1_1', 'conv1_2'], 'pool1', ['conv2_1', 'conv2_2'], 'pool2',\n",
        "    ['conv3_1', 'conv3_2', 'conv3_3', 'conv3_4'], 'pool3', ['conv4_1', 'conv4_2', 'conv4_3', 'conv4_4'], 'pool4',\n",
        "    ['conv5_1', 'conv5_2', 'conv5_3', 'conv5_4'], 'pool5', 'flatten', 'fc1_relu', 'fc2_relu', 'outputs'\n",
        "]\n",
        "\n",
        "cfg = {\n",
        "    'A': [[64], 'M', [128], 'M', [256, 256], 'M', [512, 512], 'M', [512, 512], 'M', 'F', 'fc1', 'fc2', 'O'],\n",
        "    'B': [[64, 64], 'M', [128, 128], 'M', [256, 256], 'M', [512, 512], 'M', [512, 512], 'M', 'F', 'fc1', 'fc2', 'O'],\n",
        "    'D':\n",
        "        [\n",
        "            [64, 64], 'M', [128, 128], 'M', [256, 256, 256], 'M', [512, 512, 512], 'M', [512, 512, 512], 'M', 'F',\n",
        "            'fc1', 'fc2', 'O'\n",
        "        ],\n",
        "    'E':\n",
        "        [\n",
        "            [64, 64], 'M', [128, 128], 'M', [256, 256, 256, 256], 'M', [512, 512, 512, 512], 'M', [512, 512, 512, 512],\n",
        "            'M', 'F', 'fc1', 'fc2', 'O'\n",
        "        ],\n",
        "}\n",
        "\n",
        "mapped_cfg = {\n",
        "    'vgg11': 'A',\n",
        "    'vgg11_bn': 'A',\n",
        "    'vgg13': 'B',\n",
        "    'vgg13_bn': 'B',\n",
        "    'vgg16': 'D',\n",
        "    'vgg16_bn': 'D',\n",
        "    'vgg19': 'E',\n",
        "    'vgg19_bn': 'E'\n",
        "}\n",
        "\n",
        "model_urls = {\n",
        "    'vgg16': 'https://git.openi.org.cn/attachments/760835b9-db71-4a00-8edd-d5ece4b6b522?type=0',\n",
        "    'vgg19': 'https://git.openi.org.cn/attachments/503c8a6c-705f-4fb6-ba18-03d72b6a949a?type=0'\n",
        "}\n",
        "\n",
        "model_saved_name = {'vgg16': 'vgg16_weights.npz', 'vgg19': 'vgg19.npy'}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "\n",
        "    def __init__(self, layer_type, batch_norm=False, end_with='outputs', name=None):\n",
        "        super(VGG, self).__init__()\n",
        "        self.end_with = end_with\n",
        "\n",
        "        config = cfg[mapped_cfg[layer_type]]\n",
        "        self.make_layer = make_layers(config, batch_norm, end_with)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        inputs : tensor\n",
        "            Shape [None, 224, 224, 3], value range [0, 1].\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = inputs * 255. - torch.as_tensor(np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape(-1,1,1)).to(device)\n",
        "        out = self.make_layer(inputs)\n",
        "        return out\n",
        "\n",
        "\n",
        "def make_layers(config, batch_norm=False, end_with='outputs'):\n",
        "    layer_list = OrderedDict()\n",
        "    is_end = False\n",
        "    for layer_group_idx, layer_group in enumerate(config):\n",
        "        if isinstance(layer_group, list):\n",
        "            for idx, layer in enumerate(layer_group):\n",
        "                layer_name = layer_names[layer_group_idx][idx]\n",
        "                n_filter = layer\n",
        "                if idx == 0:\n",
        "                    if layer_group_idx > 0:\n",
        "                        in_channels = config[layer_group_idx - 2][-1]\n",
        "                    else:\n",
        "                        in_channels = 3\n",
        "                else:\n",
        "                    in_channels = layer_group[idx - 1]\n",
        "                layer_list[layer_name+str(idx)] = nn.Sequential(\n",
        "                        nn.Conv2d(in_channels=in_channels, out_channels=n_filter, kernel_size=(3, 3), stride=(1, 1), padding=1),\n",
        "                        nn.ReLU()\n",
        "                        )\n",
        "                if batch_norm:\n",
        "                    layer_list[layer_name+\"_batch_norm\"] = nn.BatchNorm(num_features=n_filter)\n",
        "                if layer_name == end_with:\n",
        "                    is_end = True\n",
        "                    break\n",
        "        else:\n",
        "            layer_name = layer_names[layer_group_idx]\n",
        "            if layer_group == 'M':\n",
        "                layer_list[layer_name] = nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=1)\n",
        "            elif layer_group == 'O':\n",
        "                layer_list[layer_name] = nn.Linear(out_features=1000, in_features=4096)\n",
        "            elif layer_group == 'F':\n",
        "                layer_list[layer_name] = nn.Flatten(name='flatten')\n",
        "            elif layer_group == 'fc1':\n",
        "                layer_list[layer_name] = nn.Sequential(\n",
        "                      nn.Linear(out_features=4096, in_features=512 * 7 * 7),\n",
        "                      nn.ReLU()\n",
        "                    )\n",
        "            elif layer_group == 'fc2':\n",
        "                layer_list[layer_name] = nn.Sequential(\n",
        "                      nn.Linear(out_features=4096, in_features=4096),\n",
        "                      nn.ReLU()\n",
        "                    )\n",
        "            if layer_name == end_with:\n",
        "                is_end = True\n",
        "        if is_end:\n",
        "            break\n",
        "\n",
        "    return nn.Sequential(layer_list)\n",
        "\n",
        "def restore_model(model, layer_type):\n",
        "    # download weights\n",
        "    weights = []\n",
        "    if layer_type == 'vgg16':\n",
        "        npz = np.load(os.path.join('model', model_saved_name[layer_type]), allow_pickle=True)\n",
        "        # get weight list\n",
        "        for val in sorted(npz.items()):\n",
        "            weights.append(val[1])\n",
        "            if len(list(list(model.children())[0].children())) == len(weights):\n",
        "                break\n",
        "    elif layer_type == 'vgg19':\n",
        "        npz = np.load(os.path.join('model', model_saved_name[layer_type]), allow_pickle=True, encoding='latin1').item()\n",
        "        # get weight list\n",
        "        for val in sorted(npz.items()):\n",
        "            weights.extend(val[1])\n",
        "            if len(list(list(model.children())[0].children())) == len(weights):\n",
        "                break\n",
        "    # assign weight values\n",
        "    for i in range(len(weights)):\n",
        "        if len(weights[i].shape) == 4:\n",
        "            weights[i] = np.transpose(weights[i], axes=[3, 2, 0, 1])\n",
        "    model.weights = weights\n",
        "    del weights\n",
        "\n",
        "\n",
        "def vgg19(pretrained=False, end_with='outputs', mode='dynamic', name=None):\n",
        "    \"\"\"Pre-trained VGG19 model.\n",
        "\n",
        "    Parameters\n",
        "    ------------\n",
        "    pretrained : boolean\n",
        "        Whether to load pretrained weights. Default False.\n",
        "    end_with : str\n",
        "        The end point of the model. Default ``fc3_relu`` i.e. the whole model.\n",
        "    mode : str.\n",
        "        Model building mode, 'dynamic' or 'static'. Default 'dynamic'.\n",
        "    name : None or str\n",
        "        A unique layer name.\n",
        "\n",
        "    Examples\n",
        "    ---------\n",
        "    Classify ImageNet classes with VGG19, see `tutorial_models_vgg.py <https://github.com/tensorlayer/TensorLayerX/blob/main/examples/model_zoo/vgg.py>`__\n",
        "    With TensorLayerx\n",
        "\n",
        "    >>> # get the whole model, without pre-trained VGG parameters\n",
        "    >>> vgg = vgg19()\n",
        "    >>> # get the whole model, restore pre-trained VGG parameters\n",
        "    >>> vgg = vgg19(pretrained=True)\n",
        "    >>> # use for inferencing\n",
        "    >>> output = vgg(img)\n",
        "    >>> probs = tlx.ops.softmax(output)[0].numpy()\n",
        "\n",
        "    \"\"\"\n",
        "    if mode == 'dynamic':\n",
        "        model = VGG(layer_type='vgg19', batch_norm=False, end_with=end_with, name=name)\n",
        "    elif mode == 'static':\n",
        "        raise NotImplementedError\n",
        "    else:\n",
        "        raise Exception(\"No such mode %s\" % mode)\n",
        "    if pretrained:\n",
        "        restore_model(model, layer_type='vgg19')\n",
        "    return model\n",
        "\n",
        "\n",
        "VGG19 = vgg19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPO83DwUZKoE",
        "outputId": "a4a31613-040a-4419-a262-43cc47b29a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to train_data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 344862509/344862509 [00:20<00:00, 16613703.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting train_data/flowers-102/102flowers.tgz to train_data/flowers-102\n",
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to train_data/flowers-102/imagelabels.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 502/502 [00:00<00:00, 160275.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to train_data/flowers-102/setid.mat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14989/14989 [00:00<00:00, 30848097.48it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset Flowers102\n",
              "    Number of datapoints: 1020\n",
              "    Root location: train_data\n",
              "    split=train"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "datasets.Flowers102('train_data',download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckPeLJYGQqcN"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0'\n",
        "\n",
        "###====================== HYPER-PARAMETERS ===========================###\n",
        "batch_size = 4\n",
        "n_epoch_init = 10\n",
        "n_epoch = 2000\n",
        "# create folders to save result images and trained models\n",
        "save_dir = \"samples\"\n",
        "if not os.path.isdir('/content/'+save_dir):\n",
        "  os.makedirs('/content/'+save_dir)\n",
        "checkpoint_dir = \"models\"\n",
        "if not os.path.isdir('/content/'+checkpoint_dir):\n",
        "  os.makedirs('/content/'+checkpoint_dir)\n",
        "\n",
        "hr_transform = transforms.Compose([\n",
        "    transforms.Resize(size = (384, 384)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "])\n",
        "nor = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "lr_transform = transforms.Resize(size=(96, 96))\n",
        "\n",
        "train_hr_imgs = [img for img in os.walk('/content/train_data/flowers-102/jpg/')]\n",
        "\n",
        "\n",
        "class TrainData(Dataset):\n",
        "\n",
        "    def __init__(self, hr_trans=hr_transform, lr_trans=lr_transform):\n",
        "        self.train_hr_imgs = train_hr_imgs[0][2:][0]\n",
        "        self.hr_trans = hr_trans\n",
        "        self.lr_trans = lr_trans\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open('/content/train_data/flowers-102/jpg/'+self.train_hr_imgs[index])\n",
        "        hr_patch = self.hr_trans(img)\n",
        "        lr_patch = self.lr_trans(hr_patch)\n",
        "\n",
        "        return nor(lr_patch), nor(hr_patch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_hr_imgs)\n",
        "\n",
        "\n",
        "class WithLoss_init(nn.Module):\n",
        "    def __init__(self, vgg, G_net, loss_fn):\n",
        "        super(WithLoss_init, self).__init__()\n",
        "        self.net = G_net\n",
        "        self.vgg = vgg\n",
        "        self.loss_fn = loss_fn\n",
        "        self.counter = 0\n",
        "        self.trans = transforms.Compose([\n",
        "            transforms.Resize(size = (224, 224)),\n",
        "        ])\n",
        "\n",
        "    def forward(self, lr, hr):\n",
        "        out = self.net(lr)\n",
        "        if self.counter == 50:\n",
        "          plt.imshow(out[0].detach().cpu().squeeze().permute(1,2,0))\n",
        "          plt.show()\n",
        "          self.counter = 0\n",
        "        else:\n",
        "          self.counter += 1\n",
        "        feature_fake = self.vgg(self.trans(out))\n",
        "        feature_real = self.vgg(self.trans(hr))\n",
        "        loss = self.loss_fn(out, hr)\n",
        "        vgg_loss = 100000 * self.loss_fn(feature_fake, feature_real)\n",
        "        return loss + vgg_loss\n",
        "\n",
        "\n",
        "class WithLoss_D(nn.Module):\n",
        "    def __init__(self, D_net, G_net, loss_fn):\n",
        "        super(WithLoss_D, self).__init__()\n",
        "        self.D_net = D_net\n",
        "        self.G_net = G_net\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def forward(self, lr, hr):\n",
        "        fake_patchs = self.G_net(lr)\n",
        "        logits_fake = self.D_net(fake_patchs)\n",
        "        logits_real = self.D_net(hr)\n",
        "        d_loss1 = self.loss_fn(logits_real, torch.ones_like(logits_real))\n",
        "        d_loss1 = torch.mean(d_loss1)\n",
        "        d_loss2 = self.loss_fn(logits_fake, torch.zeros_like(logits_fake))\n",
        "        d_loss2 = torch.mean(d_loss2)\n",
        "        d_loss = d_loss1 + d_loss2\n",
        "        return d_loss\n",
        "\n",
        "\n",
        "class WithLoss_G(nn.Module):\n",
        "    def __init__(self, vgg, D_net, G_net, loss_fn1, loss_fn2):\n",
        "        super(WithLoss_G, self).__init__()\n",
        "        self.D_net = D_net\n",
        "        self.G_net = G_net\n",
        "        self.vgg = vgg\n",
        "        self.loss_fn1 = loss_fn1\n",
        "        self.loss_fn2 = loss_fn2\n",
        "        self.counter = 0\n",
        "        self.trans = transforms.Compose([\n",
        "            transforms.Resize(size = (224, 224)),\n",
        "        ])\n",
        "\n",
        "    def forward(self, lr, hr):\n",
        "        fake_patchs = self.G_net(lr)\n",
        "        if self.counter == 200:\n",
        "          plt.imshow(fake_patchs[0].detach().cpu().squeeze().permute(1,2,0))\n",
        "          plt.show()\n",
        "          self.counter = 0\n",
        "        else:\n",
        "          self.counter += 1\n",
        "        logits_fake = self.D_net(fake_patchs)\n",
        "        # feature_fake = self.vgg(self.trans(fake_patchs))\n",
        "        # feature_real = self.vgg(self.trans(hr))\n",
        "\n",
        "        g_gan_loss = self.loss_fn1(logits_fake, torch.ones_like(logits_fake))\n",
        "        g_gan_loss = torch.mean(g_gan_loss)\n",
        "        mse_loss = self.loss_fn2(fake_patchs, hr)\n",
        "        # vgg_loss = 1000 * self.loss_fn2(feature_fake, feature_real)\n",
        "        g_loss = mse_loss + g_gan_loss #+ vgg_loss\n",
        "        return g_loss\n",
        "\n",
        "\n",
        "G = SRGAN_g()\n",
        "D = SRGAN_d()\n",
        "G = G.to(device)\n",
        "D = D.to(device)\n",
        "VGG = VGG19(pretrained=True, end_with='pool4', mode='dynamic')\n",
        "VGG = VGG.to(device)\n",
        "\n",
        "\n",
        "def train():\n",
        "    G.train()\n",
        "    D.train()\n",
        "    VGG.eval()\n",
        "    train_ds = TrainData()\n",
        "    train_ds = [train_ds.__getitem__(i) for i in range(40)]\n",
        "    train_ds_img_nums = len(train_ds)\n",
        "    train_ds = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "    g_optimizer = optim.Adam(G.parameters(), lr=0.0001)\n",
        "    scheduler_g = optim.lr_scheduler.ExponentialLR(g_optimizer, gamma=0.5)\n",
        "    d_optimizer = optim.Adam(D.parameters(), lr=0.0001)\n",
        "    scheduler_d = optim.lr_scheduler.ExponentialLR(d_optimizer, gamma=0.5)\n",
        "    g_weights = G.parameters()\n",
        "    d_weights = D.parameters()\n",
        "    net_with_loss_init = WithLoss_init(VGG, G, loss_fn=F.mse_loss)\n",
        "    net_with_loss_D = WithLoss_D(D_net=D, G_net=G, loss_fn=F.binary_cross_entropy)\n",
        "    criterion_D = F.cross_entropy\n",
        "    net_with_loss_G = WithLoss_G(vgg=VGG, D_net=D, G_net=G, loss_fn1=F.binary_cross_entropy,\n",
        "                                 loss_fn2=F.mse_loss)\n",
        "\n",
        "    # initialize learning (G)\n",
        "    n_step_epoch = round(train_ds_img_nums // batch_size)\n",
        "    for epoch in range(n_epoch_init):\n",
        "        for step, (lr_patch, hr_patch) in enumerate(train_ds):\n",
        "            step_time = time.time()\n",
        "            g_optimizer.zero_grad()\n",
        "\n",
        "            loss = net_with_loss_init(lr_patch.to(device).float(), hr_patch.to(device).float())\n",
        "            loss.backward()\n",
        "            g_optimizer.step()\n",
        "            print(\"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, mse: {:.7f} \".format(\n",
        "                epoch, n_epoch_init, step, n_step_epoch, time.time() - step_time, float(loss)))\n",
        "\n",
        "    # adversarial learning (G, D)\n",
        "    n_step_epoch = round(train_ds_img_nums // batch_size)\n",
        "    for epoch in range(n_epoch):\n",
        "        for step, (lr_patch, hr_patch) in enumerate(train_ds):\n",
        "            step_time = time.time()\n",
        "            g_optimizer.zero_grad()\n",
        "            d_optimizer.zero_grad()\n",
        "            loss_g = net_with_loss_G(lr_patch.to(device).float(), hr_patch.to(device).float())\n",
        "            loss_g.backward()\n",
        "            g_optimizer.step()\n",
        "            loss_d = net_with_loss_D(lr_patch.to(device).float(), hr_patch.to(device).float())\n",
        "            loss_d.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            print(\n",
        "                \"Epoch: [{}/{}] step: [{}/{}] time: {:.3f}s, g_loss:{:.7f}, d_loss: {:.7f}\".format(\n",
        "                    epoch, n_epoch, step, n_step_epoch, time.time() - step_time, float(loss_g), float(loss_d)))\n",
        "\n",
        "        if epoch % 250 == 0:\n",
        "          scheduler_g.step()\n",
        "          scheduler_d.step()\n",
        "\n",
        "        if epoch != 0 and epoch % 250 == 0 or epoch == n_epoch-1:\n",
        "            torch.save(G.state_dict(), os.path.join(checkpoint_dir, 'g.npz'))\n",
        "            torch.save(D.state_dict(), os.path.join(checkpoint_dir, 'd.npz'))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument('--mode', type=str, default='train', help='train, eval')\n",
        "\n",
        "    args = parser.parse_known_args()\n",
        "\n",
        "    flag = args[0].mode\n",
        "\n",
        "    if flag == 'train':\n",
        "        train()\n",
        "    elif flag == 'eval':\n",
        "        evaluate()\n",
        "    else:\n",
        "        raise Exception(\"Unknow --mode\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8U0BJ1IP0xMM"
      },
      "outputs": [],
      "source": [
        "torch.save(G.state_dict(), os.path.join(checkpoint_dir, 'g.npz'))\n",
        "torch.save(D.state_dict(), os.path.join(checkpoint_dir, 'd.npz'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqIv0s7VDyfi"
      },
      "outputs": [],
      "source": [
        "def evaluate():\n",
        "    ###====================== PRE-LOAD DATA ===========================###\n",
        "    # valid_hr_imgs = Image.open(path='/content/seg_test/seg_test/street/')\n",
        "    ###========================LOAD WEIGHTS ============================###\n",
        "    G = SRGAN_g()\n",
        "    G = G.to(device)\n",
        "    G.load_state_dict(torch.load(os.path.join('/content/models', 'g.npz')))\n",
        "    G.eval()\n",
        "    valid_hr_img = Image.open('/content/train_data/flowers-102/jpg/image_00002.jpg')\n",
        "    valid_lr_img = np.asarray(valid_hr_img)\n",
        "    hr_size1 = [valid_lr_img.shape[0], valid_lr_img.shape[1]]\n",
        "\n",
        "    my_trans = transforms.Resize(size=(96,96))\n",
        "\n",
        "    nor = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    valid_lr_img = np.asarray(my_trans(valid_hr_img))\n",
        "    valid_lr_img_tensor = nor(valid_lr_img)\n",
        "\n",
        "    valid_lr_img_tensor = np.asarray(valid_lr_img_tensor, dtype=np.float32)\n",
        "    valid_lr_img_tensor = valid_lr_img_tensor[np.newaxis, :, :, :]\n",
        "    valid_lr_img_tensor= torch.as_tensor(valid_lr_img_tensor)\n",
        "    size = [valid_lr_img.shape[0], valid_lr_img.shape[1]]\n",
        "    out = np.array(G(valid_lr_img_tensor.to(device).float()).detach().cpu())\n",
        "    out = np.asarray(out * 255, dtype=np.uint8)\n",
        "    out = np.transpose(out[0], axes=[1, 2, 0])\n",
        "    print(\"LR size: %s /  generated HR size: %s\" % (size, out.shape))  # LR size: (339, 510, 3) /  gen HR size: (1, 1356, 2040, 3)\n",
        "    print(\"[*] save images\")\n",
        "    Image.fromarray(out).save(save_dir + '/valid_gen.png')\n",
        "    Image.fromarray(valid_lr_img).save(save_dir + '/valid_lr.png')\n",
        "    valid_hr_img.save(save_dir + '/valid_hr.png')\n",
        "    out_bicu = cv2.resize(valid_lr_img, dsize = [size[1] * 4, size[0] * 4], interpolation = cv2.INTER_CUBIC)\n",
        "    Image.fromarray(out_bicu).save(save_dir + '/valid_hr_cubic.png')\n",
        "\n",
        "\n",
        "evaluate()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}